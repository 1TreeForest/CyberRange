<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<!-- DW6 -->
<head>
<!-- Copyright 2005 Macromedia, Inc. All rights reserved. -->
<title>Ivan Laptev > Projects > Human Action Classification</title>
<meta http-equiv="Content-Type" content="text/html; charset=iso-8859-1" />
<link rel="stylesheet" href="mm_training.css" type="text/css" />
<style type="text/css">
<!--
.style1 {color: #FF0000}
.style4 {font-size: 12px; }
.style6 {
	font-size: 16px;
	font-weight: bold;
}
.style7 {color: #26354A}
.style9 {color: #26354A; font-size: 12px; }
.Style10 {font-size: 14px}
-->
</style>
<meta name="projects" content="" />
</head>
<!---
<body bgcolor="#64748B">
--->
<body bgcolor="#D3DCE6">
<table width="100%"  border="0" cellpadding="0" cellspacing="0">
	<tr bgcolor="#26354A">
	<td width="56" nowrap="nowrap"><img src="mm_spacer.gif" alt="" width="15" height="1" border="0" /></td>
	<td height="70" colspan="3" nowrap="nowrap" class="logo">Ivan Laptev <span class="tagline">| IRISA/INRIA Rennes France </span></td>
	</tr>

	<tr bgcolor="#FF6600">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="4" border="0" /></td>
	</tr>

	<tr bgcolor="#D3DCE6">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	</tr>

	<tr bgcolor="#FFCC00">
	<td width="56" nowrap="nowrap">&nbsp;</td>
	<td colspan="3" height="24">
	<table width="457" border="0" cellpadding="0" cellspacing="0" id="navigation">
        <tr>
          <td width="60" align="center" nowrap="nowrap" class="navText"><a href="../index.html">HOME</a></td>
          <td width="83" align="center" nowrap="nowrap" class="navText"><a href="../index.html#projectspart">PROJECTS</a></td>
          <td width="107" align="center" nowrap="nowrap" class="navText"><a href="../publications.html">PUBLICATIONS</a></td>
          <td width="88" align="center" nowrap="nowrap" class="navText"><a href="../download.html">DOWNLOAD</a></td>
          <td width="75" align="center" nowrap="nowrap" class="navText"><a href="../contact.html">CONTACT</a></td>
          <td width="59" align="center" nowrap="nowrap" class="navText"><a href="javascript:;">CVART</a></td>
          <td width="50" align="center" nowrap="nowrap" class="navText">&nbsp;</td>
        </tr>
      </table>	</td>
	</tr>

	<tr bgcolor="#D3DCE6">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	</tr>

	<tr bgcolor="#FF6600">
	<td height="4" colspan="4"></td>
	</tr>
	

	<tr bgcolor="#D3DCE6">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	<!---
	<td colspan="2" bgcolor="#64748B"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	--->
	</tr>



	<tr bgcolor="#D3DCE6">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	<!---
	<td colspan="2" bgcolor="#64748B"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	--->
	</tr>

	<tr>
	  <td colspan="3" bgcolor="#D3DCE6">&nbsp;</td>
	  <td height="180" bgcolor="#D3DCE6"><table width="889" height="227" border="0" cellpadding="0" cellspacing="0">
        <tr>
          <td colspan="7" nowrap="nowrap" bgcolor="#D3DCE6" class="logo style1"><blockquote>
              <p align="center"><strong>Learning Human Actions from Movies</strong></p>
          </blockquote></td>
        </tr>

		
        <tr>
          <td colspan="5">
		  <p align="center" class="style4 bodyText"><strong> Joint work by 		
		  <span class="bodyText style4"><a href="../index.html" class="style1">I.Laptev</a></span>,
		  <span class="bodyText style4"><a href="http://lear.inrialpes.fr/people/marszalek/" class="style1">M.Marsza&#322;ek</a></span>,
		  <span class="bodyText style4"><a href="http://lear.inrialpes.fr/people/schmid/" class="style1">C.Schmid</a></span>,
		  and B.Rozenfeld </strong></p>		  </td>
        </tr>
        <tr>
          <td colspan="5" class="pageName">
		  <p align="center"><span class="pageName">[</span><span class="pageName style6 Style10">Download:</span>
		  <span class="bodyText style4"><a href="http://www.irisa.fr/vista/Papers/2008_cvpr_laptev.pdf" class="style1">CVPR08 paper</a></span>	  
		  <span class="pageName">|</span>
		  <span class="bodyText style4"><a href="http://lear.inrialpes.fr/people/marszalek/data/hoha/hollywood.tar.gz" class="style1">Movie actions dataset</a></span> 
		  <span class="pageName">|</span>
		  <span class="bodyText style4"><a href="http://lear.inrialpes.fr/people/marszalek/data/hoha/readme.txt" class="style1">Dataset description</a></span>
		  <span class="pageName">|</span>
		  <span class="bodyText style4"><a href="../download.html#stip" class="style1">STIP features code</a></span>
		  
		  <span class="pageName">]</span>
		  </p></td>
        </tr>
		
        <tr>

          <td colspan="4" class="pageName">
		  
		  <table width="100%"  border="0" cellpadding="0" cellspacing="0">
          <tr>
            <td colspan="3">&nbsp;
			<p align="left" class="bodyText style4">
			<span class="pageName">The</span> goal of this work is to recognize <em>realistic</em> 
		    human actions in <em>unconstrained videos</em> such as in feature films, sitcoms, or news segments. Our contributions concern 
			(i) automatic collection of realistic samples of human actions from movies based on movie scripts; 
			(ii) automatic learning and recognition of complex action classes using space-time interest points and a multi-channel SVM classifier 
			(iii) Improved results for human action recognition (we achieve 91.8%) on the 			
		      <span class="bodyText style4">
		      <a href="http://www.nada.kth.se/cvap/actions/" class="style1">
		      KTH actions</a></span> dataset.			  </p>			  </td>
          </tr>  
          <tr>
            <td colspan="3">&nbsp;
			<p><span class="pageName style6">Automatic action annotation</p>
			<p align="left" class="bodyText style4">
			Currently existing datasets for visual human action recognition (e.g.
			  <span class="bodyText style4">
			  <a href="http://www.nada.kth.se/cvap/actions/" class="style1">
		      KTH actions</a></span> 
			  dataset)  provide samples for only a few action classes
              recorded in controlled and simplified settings.
			  We address this limitation and collect realistic video samples with human actions
              as illustrated on the right. We avoid the difficulty of manual video annotation 
			  and generate automatic action annotation from movie scripts.			</p>

			<p align="left" class="bodyText style4">		
			Movie scripts (
			  <span class="bodyText style4">
			  <a href="http://www.dailyscript.com/movie.html" class="style1">
		      www.dailyscript.com</a></span>, 
			  <span class="bodyText style4">
			  <a href="http://www.movie-page.com/movie_scripts.htm" class="style1">
		      www.movie-page.com</a></span>, 
			  <span class="bodyText style4">
			  <a href="http://www.weeklyscript.com/movies.htm" class="style1">
		      www.weeklyscript.com</a></span>) provide
			  text description of the movie content in terms of scenes,
              characters, transcribed dialogs and human actions.
			  To obtain temporal alignment of scripts with the video
			  we use movie subtitles with time information and
			  (i) align speech sections between subtitles and scripts;
			  (ii) transfer time information to scene descriptions in scripts as illustrated below.			</p>
			
			<p align="center">
			<img src="scriptalignment.png" width="544" height="241" />			</p>

			<p align="left" class="bodyText style4">		
			To retrieve human action samples from scripts,
			we train a Regularized Perceptron <em> text classifier </em> for each
			action class and use it to assign action labels to scene 
			descriptions.  The use of a text action classifier has shown to
			be crucial as illustrated below by comparison of action retrieval achieved by the Regularized Perceptron and by the hand-tuned regular expression matching.			</p>
			
			<p align="center">
			<img src="textclassification.png" width="544" height="229" />			</p>			</td>
          </tr>   
		  </table>		  </td>
		  
		  <td width="214" colspan="-1" class="pageName"><p align="right" class="bodyText style4">
			<img src="kiss02.jpg" width="198" height="133" />
			<img src="bgcolorimg.jpg" width="198" height="3" />
		    <img src="kiss01.jpg" width="198" height="133" />
			<img src="bgcolorimg.jpg" width="198" height="3" />
		    <img src="answerphone02.jpg" width="198" height="133" />
			<img src="bgcolorimg.jpg" width="198" height="3" />
		    <img src="answerphone04.jpg" width="198" height="133" />
			<img src="bgcolorimg.jpg" width="198" height="3" />
		    <img src="getoutcar01.jpg" width="198" height="133" />
			<img src="bgcolorimg.jpg" width="198" height="3" />
		    <img src="getoutcar02.jpg" width="198" height="133" />
			<img src="bgcolorimg.jpg" width="198" height="3" />
		    <img src="getoutcar04.jpg" width="198" height="133" />
	        </p>
		    <p>&nbsp;</p>			</td>
        	</tr>
		        <tr>
		          <td colspan="5" class="pageName">&nbsp;
				  <p><span class="pageName style6">Video representation </p>				  </td>
       			</tr>
		        <tr>
		          <td colspan="2" class="pageName">
				  <img src="actionfeatures.jpg" width="346" height="243" />				  </td>
		          <td colspan="3" class="pageName">
				  <p align="left" class="bodyText style4">
				  We describe each video segment using 
			      <span class="bodyText style4">
			      <a href="../interestpoints.html" class="style1">
		          Space-time interest points (STIP)</a></span>.
				  Interest points are detected for a fixed set of multiple spatio-temporal scales
				  as illustrated on the left for a sample of the HandShake action.
				  For each interest point we compute descriptors of the associated space-time
				  patch. We compute two alternative patch descriptors in terms of (i) histograms of
				  oriented (spatial) gradient (HOG) and (ii) histograms of optical flow (HOF).
				  Our local descriptors concatenate several histograms from a space-time grid defined on the
				  patch and generalize 
				    <span class="bodyText style4">
			        <a href="http://www.cs.ubc.ca/~lowe/keypoints/" class="style1">
		            SIFT descriptor</a></span>
				  to space-time.
				  We build a visual vocabulary of local space-time descriptors and assign each
				  interest point to a visual word label.
				  <br><br>		      
				  </p>				  </td>
                </tr>
		        <tr>
		          <td colspan="2" class="pageName">
				  <p align="left" class="bodyText style4">
				  We represent each video sequence by histograms of visual word
				  occurrences over a space-time volume corresponding either
				  to the entire video sequence (Bag-of-Features (BoF) representation) or 
				  multiple subsequences defined by a video grid.
				  A few of our video grids are illustrated to the right.
				  Each combination of a video grid with either HOG or HOF
				  descriptor is called a channel.
				  </p>
				  </td>
		          <td colspan="3" class="pageName">&nbsp;
				    <div align="right"><img src="spattempgrids.png" width="470" height="119" />				  </div>
				    <br>				  
				  </td>
        		</tr>
		        <tr>
		          <td colspan="5" class="pageName">&nbsp;
				  <p><span class="pageName style6">Action classification</p>				  </td>
                </tr>
		        <tr>
		          <td colspan="2" class="pageName">&nbsp;
				    <div align="left"><img src="svmchannels.png" width="333" height="219" />				  </div>
				    <br>				  
				  </td>
		          <td colspan="3" class="pageName">
				  <p align="left" class="bodyText style4">
				  For classification, we use a non-linear support vector machine
                  with a multi-channel kernel. We first select the overall most successful
				  channels and then choose the most successful combination of channels 
				  for each action class individually. Figure to the left illustrates
				  the number of occurrences of channel components in the channel 
				  combinations optimized for 
			        <span class="bodyText style4">
			        <a href="http://www.nada.kth.se/cvap/actions/" class="style1">
		            KTH actions</a></span> dataset
				  and our movie action dataset.
				  We observe that HOG descriptors are chosen more frequently than HOFs, but both
				  are used in many channels. Among the grids, BoF representations are selected most frequently for movie actions. Finer grids such as horizontal 3x1 partitioning and 3-bin temporal grids are frequently selected for the KTH dataset. The observed behavior is consistent with the fact that KTH dataset is more structured in space-time compared to our movie actions dataset.</p>
				  </td>
	            </tr>
		        <tr>
		          <td colspan="5" class="pageName">&nbsp;
				  <p><span class="pageName style6">Classification results</p>				  </td>
                </tr>
		        <tr>
		          <td colspan="2" class="pageName">
				  <p align="left" class="bodyText style4">
				  We validate our action recognition method on
			        <span class="bodyText style4">
			        <a href="http://www.nada.kth.se/cvap/actions/" class="style1">
		            KTH actions</a></span> dataset.
				  The results to the right demonstrate a significant improvement achieved by
				  our method when compared to other recently reported results on the same
				  dataset and using the same experimental settings.				  </p>				  </td>
		          <td colspan="3" class="pageName">
				    <div align="right"><img src="results_kth.png" width="490" height="60" />				  </div>
				    <br>					</td>
                </tr>
		        <tr>
		          <td colspan="3" class="pageName">&nbsp;
				  <p align="left" class="bodyText style4">
				  We report recognition results for 8 classes of realistic human actions when training on action
				  samples from 12 movies and testing on samples from 20 different movies. 
				  We used two alternative training sets. The 
				  <em>Automatic</em> training set was constructed using automatic action annotation method (see above)
				  and contains over 60% correct action labels. The <em>Clean</em> training set was obtained by
				  manually correcting the Automatic set. Recognition results for both training sets when using
				  the same set for testing are illustrated to the right and show significantly better performance compared to
				  chance. Our annotated human actions dataset
				    <span class="bodyText style4">
		 			<a href="http://lear.inrialpes.fr/people/marszalek/data/hoha/hollywood.tar.gz" class="style1">
		  			HOHA (2.4Gb)</a>					</span> 
				  is available for download. </p>				  </td>
		          <td colspan="2" class="pageName">
				    <div align="center"><img src="results_hoha.png" width="354" height="169" />				  </div>				  </td>
	            </tr>
		        <tr>
		          <td class="pageName">&nbsp;</td>
		          <td class="pageName">&nbsp;</td>
		          <td class="pageName">&nbsp;</td>
		          <td class="pageName">&nbsp;</td>
		          <td colspan="-1" class="pageName">&nbsp;</td>
       			</tr>
		        <tr>
          		  <td colspan="5" class="pageName">&nbsp;
				  <p><span class="pageName style6">Action classification demo</span></p>
                  <p align="center">
		          <embed src="http://video.google.com/googleplayer.swf?docId=3408448824412371791&hl=en" 
				  style="width:640px; height:480px;" id="VideoPlayback" type="application/x-shockwave-flash" 
				  flashvars="" name="VideoPlayback">				  </embed>	
				  <br>
				  <span class="bodyText style4">See mpeg version of the demo 
				  <a href="cvpr08demo.mpg" class="style1">here</a>.
				  </span>
				  </p>
				  </td>
        </tr>
        <tr>
          <td width="163" class="pageName">&nbsp;</td>
          <td width="219" class="pageName">&nbsp;</td>
          <td width="75" class="pageName">&nbsp;</td>
          <td width="218" class="pageName">&nbsp;</td>
          <td width="214" colspan="-1" class="pageName"><p align="right" class="bodyText style4">&nbsp;</p></td>
        </tr>
        <tr>
		<td colspan="5"><div align="center">
		  <p align="left">&nbsp;</p>
		  <p align="left" class="pageName style6">Further details   </p>
		  
		  <p align="left" class="subHeader">Paper:<br>
		      <span class="bodyText style4">
		  <a href="http://www.irisa.fr/vista/Papers/2008_cvpr_laptev.pdf" class="style1">
		    &quot;Learning Realistic Human Actions from Movies&quot;</a>,<br />
		    Ivan Laptev, Marcin Marsza&#322;ek, Cordelia Schmid and Benjamin Rozenfeld; in <i> Proc. CVPR'08</i></span></p>
			<!---
		  <p align="left" class="subHeader">Slides:<br>
			 <span class="bodyText style4">
			<a href="http://videolectures.net/mcvc08_laptev_acdr/" class="style1">
			On-line talk</a></span></p>
			--->
		  <p align="left" class="subHeader">Slides:<br>
		      <span class="bodyText style4">
			CVPR08 talk
		  <a href="cvpr08.ppt" class="style1">without videos</a> or
		  <a href="cvpr08.zip" class="style1">with videos (35Mb)</a>

		  <p align="left"><span class="subHeader">Download:<br />
		  <span class="bodyText style4">
		  <a href="../download.html#actionclassification" class="style1">
		  <!---
		  <a href="http://lear.inrialpes.fr/people/marszalek/data/hoha/hollywood.tar.gz" class="style1">
		  --->
		  <!---
		  <a href="../download/hollywood.tar.gz" class="style1">
		  --->
		  Hollywood Human Actions (HOHA) dataset (2.4Gb)</a>
		  used in CVPR08 paper. See dataset description </span> 
		  <span class="bodyText style4"><a href="http://lear.inrialpes.fr/people/marszalek/data/hoha/readme.txt" class="style1">here</a></span>
		  <span class="bodyText style4">.</span>
		  <br />

		  <span class="bodyText style4">
		  <a href="../download.html#stip" class="style1">
		  Fast implementation</a>
		  of space-time interest point detector and descriptors used in CVPR08 paper.
		  <br />
		  </span></div></td>
        </tr>
		
      </table>
        <p class="bodyText">&nbsp;</p>
      <p></p></td></tr>


		
	<tr bgcolor="#D3DCE6">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	</tr>

	




	<tr bgcolor="#D3DCE6">
	<td colspan="4"><img src="mm_spacer.gif" alt="" width="1" height="1" border="0" /></td>
	</tr>
</table>
</body>
</html>